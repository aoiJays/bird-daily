import time
import json
import csv
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

def setup_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless=new") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")

    service = ChromeService(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def run_scraper():
    driver = None
    data_list = []
    
    try:
        print("ğŸš€ å¯åŠ¨çˆ¬è™«...")
        driver = setup_driver()
        
        url = "https://www.zhihu.com/explore"
        driver.get(url)
        
        # æ˜¾å¼ç­‰å¾…ï¼šç›´åˆ°é¡µé¢ä¸­è‡³å°‘å‡ºç°ä¸€ä¸ªå†…å®¹æ ‡é¢˜ï¼ˆæœ€å¤šç­‰15ç§’ï¼‰
        # Zhihu çš„æ ‡é¢˜ class é€šå¸¸åŒ…å« 'ContentItem-title'
        print("â³ ç­‰å¾…é¡µé¢åŠ è½½...")
        wait = WebDriverWait(driver, 15)
        wait.until(EC.presence_of_element_located((By.CLASS_NAME, "ContentItem-title")))
        
        # æ¨¡æ‹Ÿæ»šåŠ¨ï¼Œè§¦å‘æ‡’åŠ è½½ï¼ˆå¦‚æœéœ€è¦æ›´å¤šæ•°æ®ï¼Œå¯ä»¥å¤šæ»šåŠ¨å‡ æ¬¡ï¼‰
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3) 

        # æŸ¥æ‰¾æ‰€æœ‰æ ‡é¢˜å…ƒç´ 
        # æ³¨æ„ï¼šçŸ¥ä¹çš„å‰ç«¯ä»£ç ç»å¸¸å˜ï¼Œå¦‚æœè¿™é‡ŒæŠ“ä¸åˆ°ï¼Œå¯èƒ½éœ€è¦æ›´æ–° Selector
        elements = driver.find_elements(By.CLASS_NAME, "ContentItem-title")
        
        print(f"âœ… æ‰¾åˆ° {len(elements)} ä¸ªå†…å®¹æ ‡é¢˜ã€‚")
        
        for index, elem in enumerate(elements, 1):
            try:
                # å°è¯•è·å–æ ‡é¢˜å†…çš„é“¾æ¥ï¼Œå¦‚æœæ²¡æœ‰é“¾æ¥åˆ™è·å–æ–‡æœ¬
                link_elem = elem.find_element(By.TAG_NAME, "a")
                title = link_elem.text
                link = link_elem.get_attribute("href")
            except:
                # å¤‡ç”¨æ–¹æ¡ˆ
                title = elem.text
                link = "N/A"
            
            print(f"{index}. {title}")
            data_list.append({"title": title, "link": link})

    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        # å‡ºé”™æ—¶ä¿å­˜æˆªå›¾ï¼Œæ–¹ä¾¿è°ƒè¯•
        if driver:
            driver.save_screenshot("error_screenshot.png")
            
    finally:
        if driver:
            driver.quit()

    # --- ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶ ---
    if data_list:
        # ä¿å­˜ä¸º JSON
        with open('zhihu_data.json', 'w', encoding='utf-8') as f:
            json.dump(data_list, f, ensure_ascii=False, indent=4)
        print("ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° zhihu_data.json")
    else:
        print("âš ï¸ æœªæŠ“å–åˆ°æœ‰æ•ˆæ•°æ®ã€‚")

if __name__ == "__main__":
    run_scraper()